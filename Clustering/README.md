<h3>Principal Component Analysis and Hierarchical Clustering on Principal Components</h3>

<p align="justify">The R package FactoMineR (v1.41) was used for the PCA [14]. All input features for the PCA were scaled to zero mean and unit variance to account for the difference in range of the features, especially with respect to the two features in absolute terms compared to the ones in terms of percentages. The first 18 PCs explained together over 80% of the variance of the data. The remaining components were assumed to mostly represent noise in the data. The PCs were used as input to the ‘hierarchical clustering on principal components’ (HCPC) function from the FactoMineR package. The Euclidean distance was used as a measure of dissimilarity and the Ward criterion for linkage. We cut the hierarchical clustering tree at various heights to see a more global down to a more specific division of the samples. The HCPC function includes a consolidation step in the form of k-means clustering [15], which uses the centroids of the hierarchical clustering as a starting point. This consolidation step was repeated a maximum of 10 times. The k-means clustering increased the variance between clusters from 17.5 to 18.9. Other advantages of this hybrid approach are that there is no need for k-means clustering to select the initial centroids at random and it reduces the sensitivity of k-means clustering to outliers. As a consequence of this step, some samples were finally assigned to a different cluster than after the hierarchical clustering. A ‘v test’, included in the FactoMineR package, was used to determine which features were significantly associated with each cluster. This test compares the mean of a particular feature in a cluster to the overall mean in the dataset. We corrected the p-values of all ‘v tests’ for multiple testing using the Benjamini-Yekutieli method. A feature is considered to be significantly associated to a cluster if the adjusted p-value < 0.05. </p>
